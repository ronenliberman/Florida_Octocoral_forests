---
title: "Modeling Florida octocoral trends: Density data"
author: "Ronen Liberman"
date: "8-2-2024"
output: 
  html_document:
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r initiate-environment, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library("knitr")
library("gridExtra")
library("ggpubr")
library("data.table")
library(glmmTMB)
library(bbmle)
library(performance)
library(ggplot2)
library(plotrix)
library(gridExtra)
library(car)
library(emmeans)
library(DHARMa)
library(RRPP)
#library(MuMIn)
library(MASS)
library(lattice)
```

```{r customize plot style , warning=FALSE, include=FALSE}
#font_import()

theme_set(theme_minimal(base_size = 12))
theme_update(
  axis.title.x = element_text(margin = margin(12, 0, 0, 0), color = "black"),
  axis.title.y = element_text(margin = margin(0, 12, 0, 0), color = "black"),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(color = "grey45", fill = NA, linewidth = 1.5),
  panel.spacing = unit(.9, "lines"),
  strip.text = element_text(size = rel(1)),
  #plot.title = element_text(size = rel(1), face = "bold", hjust = .5),
  axis.line = element_line(colour = "black"),
        strip.text.x = element_text(size = 10, colour = "black"),
        axis.text=element_text(color="black", size=10))
```

*In this markdown file I am going to work with data that was prepared in a previous script. 
I am going to clean the data a little bit, then using model selection test for the most significant parameters that influence state of octocoral population in the last 11 years*

#Data prep. for modelling
Here I filter the data so I can model density of Total population across years 2013 - 2023. The data was prepared in: "Creating_binding_all_datasets.R"

```{r include=FALSE}
#dat <- read.csv("~/Postdocing_NSU/Octocoral forests/Manuscript/all_density_and_heights_by_stationID.csv")
dat <- read.csv("~/Postdocing_NSU/Octocoral forests/Manuscript/scripts/processed_data/all_density_and_heights_by_stationID.csv")
head(dat)

levels(factor(dat$Subregion))
levels(factor(dat$Habitat))
levels(factor(dat$Site_name))

# Here i test what should be the rangesof the data depth-groups based on number of rows 
# Filter the data for SPP_CODE == "TOCT"
TOCT_data <- dat %>% 
  filter(SPP_Code == "TOCT")

# Convert Depth to numeric (if not already done)
TOCT_data$Depth <- as.numeric(as.character(TOCT_data$Depth))

# Plot the distribution of data by Depth
ggplot(TOCT_data, aes(x = Depth)) +
  geom_bar(fill = "lightblue", color = "black") +
  labs(title = "Distribution of Data by Depth for SPP_CODE - TOCT", x = "Depth", y = "Number of Rows") 


# Calculate quartiles for the Depth variable
quartiles <- TOCT_data %>%
  summarise(Q1 = quantile(Depth, 0.25),
            Median = quantile(Depth, 0.5),
            Q3 = quantile(Depth, 0.75))

# Create a frequency table of rows per depth
depth_frequency <- TOCT_data %>%
  group_by(Depth_group = cut(Depth, breaks = c(-Inf, quartiles$Q1, quartiles$Median, quartiles$Q3, Inf))) %>%
  summarise(Frequency = n())

# Rename the Depth_group variable for clarity
depth_frequency$Depth_group <- gsub("\\(|\\]", "", as.character(depth_frequency$Depth_group))

# Assign data groups according to the frequncy found above. 
dat_4groups <- dat %>% 
  mutate(Depth_group = case_when(
    Depth < 21 ~ "Shallow",
    Depth >= 21 & Depth < 28 ~ "Mid-shallow", Depth >= 28 & Depth < 45  ~ "Mid-deep",
    Depth >= 45 ~ "Deep"
  ))

#visualize
dat_4groups %>% filter(SPP_Code == "TOCT") %>%  ggplot(aes(x = Depth_group, fill = Depth_group)) +
  geom_bar() +
  labs(title = "Frequency of Observations in Each Depth Group", x = "Depth Group", y = "Frequency") 
# ok , this makes some sense. 

#Generate the dataset
Total.octo <- dat_4groups %>% filter(SPP_Code == "TOCT") 
# check the number of unique ProjectRegion 
Total.octo %>% 
  distinct(ProjectRegion ) 

# make NPSDT the only DT ProjectRegion 
Total.octo <- Total.octo %>%
  mutate(ProjectRegion = ifelse(ProjectRegion == "NPSDT/FKNMS", "NPSDT", ProjectRegion))

Total.octo %>% 
  distinct(Habitat) 

#Here I explore a bit the outlliars in my data, some zero's, some low counts in specific sites 
#test for zero density values
#Looking at density 
Total.zero <- Total.octo %>% filter(density == "0")

# Creating a list of sites that should be omitted from the datasets 
omit_sites <- c("Martin County 1" ,"Martin County 2","Davis Rock","Palm Beach 1","Red Dun Reef","Palmata Patch","Loggerhead Patch", "Texas Rock","Broward County A")

#omit_sites2 <- c("Red Dun Reef","Palmata Patch","Loggerhead Patch", "Texas Rock","Broward County A")

Total.octo_no_2012 <- dat_4groups %>% filter(SPP_Code == "TOCT") %>% 
mutate(ProjectRegion = ifelse(ProjectRegion == "NPSDT/FKNMS", "NPSDT", ProjectRegion)) %>% # Count the number of unique ProjectRegion
 filter(!(Site_name %in% omit_sites)) %>% 
  filter(Year!= "2012") %>% 
  mutate(SiteID = as.factor(SiteID),
         StationID = as.factor(StationID),
         Year =  as.factor(Year),
         counts = (10*density)
  )
```


```{r histogram, echo=FALSE}


#view(Total.octo_no_2012)
#Total.octo %>% 
 # distinct(ProjectRegion ) 

#Total.octo %>% 
 # distinct(Habitat )

# look at the frequency of counts 

hist(Total.octo_no_2012$counts, breaks=50, xlab = "density", main = NULL, freq = TRUE) 


#round density for avoiding non-integer issues with poisson or nbinom. 
Total.octo_no_2012$offset <- 10

Total.octo_no_2012$Reg.Hab <- paste(Total.octo_no_2012$ProjectRegion, "_", Total.octo_no_2012$Habitat)
Total.octo_no_2012$Reg.Hab <- as.factor(Total.octo_no_2012$Reg.Hab)
Total.octo_no_2012$Habitat <- as.factor(Total.octo_no_2012$Habitat)
Total.octo_no_2012$Subregion <- as.factor(Total.octo_no_2012$Subregion)
Total.octo_no_2012$ProjectRegion <- as.factor(Total.octo_no_2012$ProjectRegion)
Total.octo_no_2012$Depth_group <- as.factor(Total.octo_no_2012$Depth_group ) 
unique(Total.octo_no_2012$Site_name)
unique(Total.octo_no_2012$Reg.Hab)
```

Using different models I can examine the most significant factors affecting Total octocoral abundance. I am using count data and different explenatory varables. I am using a model selection method using the AIC parameter to select for the best model. 
Afterwards I am running a model validation functions on the best model. 

Modeling using NBINOM2 dist. 

```{r }

#check distribution
poisson_glmm_full_model_2013 = glmmTMB(counts ~ Year * Subregion +Depth+Habitat+(1|SiteID/StationID),
                          family = poisson, 
                          data = Total.octo_no_2012)
poisson_glmm_full_model_2013 <- update(poisson_glmm_full_model_2013, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom1_glmm_full_model_2013 = glmmTMB(counts ~ Year * Subregion +Depth+Habitat+(1|SiteID/StationID),
                          family = nbinom1,
                          data = Total.octo_no_2012)
nbinom1_glmm_full_model_2013 <- update(nbinom1_glmm_full_model_2013, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_full_model_2013 = glmmTMB(counts ~ Year * Subregion +Depth+Habitat+(1|SiteID/StationID),
                          family = nbinom2, 
                          data = Total.octo_no_2012)
nbinom_glmm_full_model_2013 <- update(nbinom_glmm_full_model_2013, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

AIC(poisson_glmm_full_model_2013, nbinom1_glmm_full_model_2013, nbinom_glmm_full_model_2013)
###nbinom2 best

```

```{r}
#nbinom_glmm_full_model <- update(nbinom_glmm_full_model, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_depthgroup_interaction_2013 <- glmmTMB(counts ~ Year * Depth+ (1|SiteID/StationID),
                      family = nbinom2, 
                      data = Total.octo_no_2012)
#nbinom_glmm_model_depthgroup_interaction <- #update(nbinom_glmm_model_depthgroup_interaction, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_depthgroup_2013 <- glmmTMB(counts ~ Year * Depth_group + (1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)
#nbinom_glmm_model_depthgroup <- update(nbinom_glmm_model_depthgroup, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_depth_2013 <- glmmTMB(counts ~ Year + Depth + (1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

nbinom_glmm_model_Subregion_2013 <- glmmTMB(counts ~ Year * Subregion+(1|SiteID/StationID),
                               family = nbinom2, 
                                data = Total.octo_no_2012)

#nbinom_glmm_model_Subregion_2013 <- update(nbinom_glmm_model_Subregion_2013, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_Subregion2_2013 <- glmmTMB(counts ~ Year + Subregion+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)
  #  nbinom_glmm_model_Subregion2 <- update(nbinom_glmm_model_Subregion2, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))                    
         
nbinom_glmm_model_Subregion3_2013 <- glmmTMB(counts ~ Year * Subregion+Depth + (1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)   
nbinom_glmm_model_Subregion3_2013 <- update(nbinom_glmm_model_Subregion3_2013, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))


nbionom_glmm_model_ProjectRegion_2013 <- glmmTMB(counts ~ Year * ProjectRegion+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)
#nbionom_glmm_model_ProjectRegion <- update(nbionom_glmm_model_ProjectRegion, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_ProjectRegion2_2013 <- glmmTMB(counts ~ Year + ProjectRegion+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)
#nbinom_glmm_model_ProjectRegion2 <- update(nbinom_glmm_model_ProjectRegion2, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

nbinom_glmm_model_ProjectRegion3_2013 <- glmmTMB(counts ~ Year * ProjectRegion+Depth+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

nbinom_glmm_model_ProjectRegion4_2013 <- glmmTMB(counts ~ Year * ProjectRegion+Habitat+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

nbinom_glmm_model_Year_2013 <- glmmTMB(counts ~ as.numeric(Year) + (1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)
#nbinom_glmm_model_Year <- update(nbinom_glmm_model_Year, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))


nbinom_glmm_model_habitat_2013 <- glmmTMB(counts ~ Year * Habitat+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)


nbinom_glmm_model_habitat2_2013 <- glmmTMB(counts ~ Year + Habitat+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

nbinom_glmm_model_habitat3_2013 <- glmmTMB(counts ~ Year + Habitat+Subregion+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

nbinom_glmm_model_null_2013 <- glmmTMB(counts ~  + 1,
                           family = nbinom2, 
                            data = Total.octo_no_2012)

#add optional model that looks at habitat within region - this was where most variation was in our previous cover models

nbinom_glmm_model_reg.hab_2013 <- glmmTMB(counts ~ Year * Reg.Hab+(1|SiteID/StationID),
                                 family = nbinom2, 
                                 data = Total.octo_no_2012)

```

```{r }
AICtab(nbinom_glmm_full_model_2013,
       nbinom_glmm_model_depthgroup_interaction_2013,nbinom_glmm_model_depthgroup_2013,
       nbinom_glmm_model_depth_2013,
    nbinom_glmm_model_Subregion_2013,nbinom_glmm_model_Subregion2_2013, nbinom_glmm_model_Subregion3_2013,
    nbionom_glmm_model_ProjectRegion_2013,nbinom_glmm_model_ProjectRegion2_2013,
    nbinom_glmm_model_ProjectRegion3_2013,nbinom_glmm_model_ProjectRegion4_2013,
    nbinom_glmm_model_Year_2013,
    nbinom_glmm_model_habitat_2013,nbinom_glmm_model_habitat2_2013,nbinom_glmm_model_habitat3_2013,
    nbinom_glmm_model_null_2013, nbinom_glmm_model_reg.hab_2013)
```

**Summary of best model**
The model that explains the variation of count data ~ Year * Region.Habitat is the FAVORITE. 

```{r echo=FALSE, warning=TRUE}
Best_model.2 = nbinom_glmm_model_Subregion_2013
summary(Best_model.2)

#when I run these I get year x habitat as the best model?? - maybe because I made things a factor? would have presumed GLMM would coerce habitat and subregion as factors from character but maybe not?
Best_model <- nbinom_glmm_model_reg.hab_2013
summary(nbinom_glmm_model_reg.hab_2013)

library(performance)
r2(Best_model) #fit is very good with the random effects

```

Model validation

**Visualizing residuals**
Plotting residuals vs. fitted shows more or less the same result.

```{r echo=FALSE}
# Check for residual pattern within groups and difference between groups     
xyplot(residuals(Best_model) ~ fitted(nbinom_glmm_model_reg.hab_2013),
       panel=function(x, y){
         panel.xyplot(x, y)
         panel.loess(x, y, span = 3)
         panel.lmline(x, y, lty = 2)  # Least squares broken line
       }
)
```
There is some high variation in the residuals - this probably creating the outliars and dispersion roblems inmymodels. lets try to reduce the outliars by removing some marginals locations. 

Validation of Best model
Here I use the DHarMA package to validate my best model 
QQ plot is on the red line, but still there is a significant result in the uniformity test. 
As it stands, the model is almost validated, but it seems some there is an indication of issues with the uniformity of the data. 

```{r echo=TRUE}
simulationOutput <- simulateResiduals(Best_model, n = 1000, plot = TRUE,quantreg = T) #this looks pretty good, uniformity slightly off, but not too bad, some evidence of heterogeneity, recalculate residuals to check
testDispersion(simulationOutput) #dispersion fine
```

Validation per group - I examine individual groups in the dataset. #recalculating residuals , as I might have too many samples for Dharma to calculate, grouping by site, Year, or Subregion

#Site - this parameter has enough data point and therefore can be a good one to look into. All parameters are non significant

```{r}
resids.site <- recalculateResiduals(simulationOutput, group = Total.octo_no_2012$SiteID)
plot(resids.site) # heterogeneity good, uniformity improved, suggests an outlier

testResiduals(resids.site) #residuals uniform, dispersion fine, evidence of outliers, try bootstrap as per Florian Hartig vignette advice
testOutliers(resids.site, type = "bootstrap") #suggests after bootstrapping that residuals are fine
testDispersion(resids.site) #dispersion fine

```
### Test for temporal autocorrelation
```{r}
res <- recalculateResiduals(simulationOutput, group = Total.octo_no_2012$Year)
testTemporalAutocorrelation(simulationOutput = res, time = unique(Total.octo_no_2012$Year))
#evidence of temporal autocorrelation (error term in year n is a function of the error in year n-1: suggest fitting an ar1 autocorrelation structure, this is present in the full residuals and the refitted residuals

```

### Include ar1 structure to account for temporal autocorrelation

```{r}

Best_model.ar1 <- glmmTMB(counts ~ Year * Reg.Hab+(1|SiteID/StationID) +
                            ar1(Year-1|StationID),
                                 family = nbinom2, data = Total.octo_no_2012)
Best_model.ar1 <- update(Best_model.ar1, control = glmmTMBControl(optCtrl=list(iter.max=1e3,eval.max=1e3)))

AIC(Best_model, Best_model.ar1) #improved AIC with ar1

summary(Best_model.ar1) #estimates ar1 correlation coefficient at 0.93 - this is very high - I suspect it's not actually needed given the random intercept specified and time being incorporated into the model, but after reading online and Zuur et al 2009 it is suggested to be incorporated when there is temporal autocorrelation. I'll check model estimates between the models with and without AR1 anyway 
#Dharma tests residuals on the fixed effects so no need to re validate model, will check acf
#FYI ar1(Year-1) and ar1(Year+0) are equivalent and give the same estimates
```

#calculate r2, assess model estimates between ar1 adn no ar1 models
```{r}
#not possible to calculate r2 with the ar1 added so compute on fitted model
r2(Best_model) #conditional r2 = 0.897, marginal  = 0.234

summary(Best_model)

summary(Best_model.ar1)
#summaries look pretty similar - best_model.ar1 has very low variance accounted for by station intercept but that's likely because it's accounted for in the ar1 temporal autocorrelation structure

```

### Post hoc analysis of fitted factors - fyi I haven't looked at the raw data. Worth doing to make sure these estimates and comparisons make sense

```{r}

#calculated estimated marginal means and outputs estimates on the log scale

emm.year.hab <- emmeans(Best_model.ar1, ~ Year|Reg.Hab) #compares years within habitat
pairs(emm.year.hab) #significant positive ratio estimate means octo abundance in year x is greater than in year y, significant negative means sig decline between year x and year y
contrast(emm.year.hab) #compares estimate for the year against the mean for that habitat
plot(emm.year.hab)

emm.hab.year <- emmeans(Best_model.ar1, ~ Reg.Hab|Year) #does the opposite, will compare habitats within years
pairs(emm.hab.year)
contrast(emm.hab.year)
plot(emm.hab.year)

### for completeness I've included the marginal emmeans for each fixed factor

emm.hab <- emmeans(Best_model.ar1, ~ Reg.Hab)
pairs(emm.hab)
contrast(emm.hab) #all pretty similar

emm.year <- emmeans(Best_model.ar1, ~ Year)
pairs(emm.year)
contrast(emm.year)

```

### NJ - you don't need the next steps

